{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 16000 # sampling rate. wav의 경우 16000Hz\n",
    "D = 5 # 음성 시간 (초)\n",
    "FRAME_LENGTH = 0.032 # frame_length = input_nfft / sr -> 20 ~ 40ms 단위여야 함\n",
    "FRAME_STRIDE = 0.016 # 16ms마다 frequency 추출\n",
    "N_MFCC = 40 # filter 개수 -> row 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy size를 맞춰주기 위한 padding 함수\n",
    "# 빈 자리는 0으로 채워줌\n",
    "pad2D = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i - a.shape[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(file_path, data_size):\n",
    "    f_array = []\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        num = i + 1    \n",
    "        a_name = file_path + str(num) + '.wav'\n",
    "\n",
    "        # 음성파일 불러오기\n",
    "        # y: 로드된 음성데이터, sr: sampling rate\n",
    "        y, sr = librosa.load(a_name, sr=SAMPLING_RATE, duration=D)\n",
    "\n",
    "        # fft: Fast Fourier Transform\n",
    "        # 연속적인 시간 관점의 음성 신호를 주파수로 변환\n",
    "        # n_fft: window 개수 = 16000 * 0.032 = 512\n",
    "        input_nfft = int(round(sr*FRAME_LENGTH))\n",
    "\n",
    "        # hop_length\n",
    "        # 음성을 얼만큼 겹쳐서 자를 것인지?\n",
    "        # (window_length - frame_stride)만큼 겹치는 것\n",
    "        # 최소 50% 겹치게 분할해야 함\n",
    "        input_stride = int(round(sr*FRAME_STRIDE))\n",
    "\n",
    "        S = librosa.feature.mfcc(y=y, sr=SAMPLING_RATE, \n",
    "                                 n_fft=input_nfft, hop_length=input_stride,\n",
    "                                 n_mfcc=N_MFCC)\n",
    "\n",
    "        #print(\"Wav length: {}, MFCC_shape: {}\".format(len(y)/sr, np.shape(S)))\n",
    "        f_array.append(pad2D(S, 300))\n",
    "    \n",
    "    f_array = np.asarray(f_array) # list -> numpy 변환\n",
    "    return f_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링 함수\n",
    "def labeling(size, label):\n",
    "    result = np.zeros(shape=(size))\n",
    "    for i in range(size):\n",
    "        result[i] = label\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 40, 300)\n"
     ]
    }
   ],
   "source": [
    "sos_1_path = 'D:\\\\졸프\\\\sound\\\\help\\\\구해줘\\\\convert+original\\\\구해줘_'\n",
    "sos_2_path = 'D:\\\\졸프\\\\sound\\\\help\\\\그만\\\\convert+original\\\\그만_'\n",
    "sos_3_path = 'D:\\\\졸프\\\\sound\\\\help\\\\도와주세요\\\\convert+original\\\\도와주세요_'\n",
    "sos_4_path = 'D:\\\\졸프\\\\sound\\\\help\\\\도와줘\\\\convert+original\\\\도와줘_'\n",
    "sos_5_path = 'D:\\\\졸프\\\\sound\\\\help\\\\사람살려\\\\convert+original\\\\사람살려_'\n",
    "sos_6_path = 'D:\\\\졸프\\\\sound\\\\help\\\\살려주세요\\\\convert+original\\\\살려주세요_'\n",
    "sos_7_path = 'D:\\\\졸프\\\\sound\\\\help\\\\살려줘\\\\convert+original\\\\살려줘_'\n",
    "\n",
    "#human_path = 'D:\\\\졸프\\\\sound\\\\human\\\\human_'\n",
    "#animal_path = 'D:\\\\졸프\\\\sound\\\\animal\\\\animal_'\n",
    "#etc_path = 'D:\\\\졸프\\\\sound\\\\etc\\\\etc_'\n",
    "\n",
    "sos_1 = get_mfcc(sos_1_path, 1194) # 구해줘\n",
    "sos_2 = get_mfcc(sos_2_path, 1178) # 그만\n",
    "sos_3 = get_mfcc(sos_3_path, 824) # 도와주세요\n",
    "sos_4 = get_mfcc(sos_4_path, 966) # 도와줘\n",
    "sos_5 = get_mfcc(sos_5_path, 955) # 사람살려\n",
    "sos_6 = get_mfcc(sos_6_path, 1340) # 살려주세요\n",
    "sos_7 = get_mfcc(sos_7_path, 1141) # 살려줘\n",
    "\n",
    "#human = get_mfcc(human_path, 43)\n",
    "#animal = get_mfcc(animal_path, 927)\n",
    "#etc = get_mfcc(etc_path, 660)\n",
    "\n",
    "print(sos_6.shape)\n",
    "#print(human.shape)\n",
    "#print(animal.shape)\n",
    "#print(etc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340,)\n"
     ]
    }
   ],
   "source": [
    "sos_1_id = labeling(sos_1.shape[0], 0)\n",
    "sos_2_id = labeling(sos_2.shape[0], 1)\n",
    "sos_3_id = labeling(sos_3.shape[0], 2)\n",
    "sos_4_id = labeling(sos_4.shape[0], 3)\n",
    "sos_5_id = labeling(sos_5.shape[0], 4)\n",
    "sos_6_id = labeling(sos_6.shape[0], 5)\n",
    "sos_7_id = labeling(sos_7.shape[0], 6)\n",
    "\n",
    "#human_id = labeling(human.shape[0], 0)\n",
    "#animal_id = labeling(animal.shape[0], 0)\n",
    "#etc_id = labeling(etc.shape[0], 0)\n",
    "print(sos_6_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5315, 40, 300)\n",
      "(2283, 40, 300)\n",
      "(5315,)\n",
      "(2283,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sos_1_train, sos_1_test, sos_1_id_train, sos_1_id_test = train_test_split(sos_1, sos_1_id, test_size=0.3, random_state=12)\n",
    "sos_2_train, sos_2_test, sos_2_id_train, sos_2_id_test = train_test_split(sos_2, sos_2_id, test_size=0.3, random_state=12)\n",
    "sos_3_train, sos_3_test, sos_3_id_train, sos_3_id_test = train_test_split(sos_3, sos_3_id, test_size=0.3, random_state=12)\n",
    "sos_4_train, sos_4_test, sos_4_id_train, sos_4_id_test = train_test_split(sos_4, sos_4_id, test_size=0.3, random_state=12)\n",
    "sos_5_train, sos_5_test, sos_5_id_train, sos_5_id_test = train_test_split(sos_5, sos_5_id, test_size=0.3, random_state=12)\n",
    "sos_6_train, sos_6_test, sos_6_id_train, sos_6_id_test = train_test_split(sos_6, sos_6_id, test_size=0.3, random_state=12)\n",
    "sos_7_train, sos_7_test, sos_7_id_train, sos_7_id_test = train_test_split(sos_7, sos_7_id, test_size=0.3, random_state=12)\n",
    "\n",
    "#human_train, human_test, human_id_train, human_id_test = train_test_split(human, human_id, test_size=0.3, random_state=12)\n",
    "#animal_train, animal_test, animal_id_train, animal_id_test = train_test_split(animal, animal_id, test_size=0.3, random_state=12)\n",
    "#etc_train, etc_test, etc_id_train, etc_id_test = train_test_split(etc, etc_id, test_size=0.3, random_state=12)\n",
    "\n",
    "x_train = np.concatenate((sos_1_train, sos_2_train, sos_3_train, sos_4_train, sos_5_train, sos_6_train, sos_7_train), axis=0)\n",
    "x_test = np.concatenate((sos_1_test, sos_2_test, sos_3_test, sos_4_test, sos_5_test, sos_6_test, sos_7_test), axis=0)\n",
    "y_train = np.concatenate((sos_1_id_train, sos_2_id_train, sos_3_id_train, sos_4_id_train, sos_5_id_train, sos_6_id_train, sos_7_id_train), axis=0)\n",
    "y_test = np.concatenate((sos_1_id_test, sos_2_id_test, sos_3_id_test, sos_4_id_test, sos_5_id_test, sos_6_id_test, sos_7_id_test), axis=0)\n",
    "\n",
    "#x_train = np.concatenate((sos_6_train, human_train, animal_train, etc_train), axis=0)\n",
    "#x_test = np.concatenate((sos_6_test, human_test, animal_test, etc_test), axis=0)\n",
    "#y_train = np.concatenate((sos_6_id_train, human_id_train, animal_id_train, etc_id_train), axis=0)\n",
    "#y_test = np.concatenate((sos_6_id_test, human_id_test, animal_id_test, etc_id_test), axis=0)\n",
    "\n",
    "# random shuffle\n",
    "train_index = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(train_index)\n",
    "\n",
    "test_index = np.arange(x_test.shape[0])\n",
    "np.random.shuffle(test_index)\n",
    "\n",
    "x_train = x_train[train_index]\n",
    "y_train = y_train[train_index]\n",
    "\n",
    "x_test = x_test[test_index]\n",
    "y_test = y_test[test_index]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5315, 40, 300, 1)\n",
      "(2283, 40, 300, 1)\n",
      "(5315,)\n",
      "(2283,)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 40, 300, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 40, 300, 1).astype('float32')\n",
    "\n",
    "#y_train = to_categorical(y_train, 7)\n",
    "#y_test = to_categorical(y_test, 7)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = './model/'\n",
    "\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "    os.mkdir(MODEL_SAVE_PATH)\n",
    "\n",
    "model_path = MODEL_SAVE_PATH + 'sos-' + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "cb_early_stopping = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "input_shape = x_train[0].shape\n",
    "\n",
    "# raspberrypi 호환성을 위해 batch_size 작게 조정\n",
    "model = Sequential()\n",
    "\n",
    "# 1st conv layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape,\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
    "\n",
    "# 2nd conv layer\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
    "\n",
    "# 3rd conv layer\n",
    "model.add(Conv2D(16, (2, 2), activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "# flatten output and feed into dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "Dropout(0.3)\n",
    "\n",
    "# softmax output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 38, 298, 32)       320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 38, 298, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 19, 149, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 17, 147, 16)       4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 17, 147, 16)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 9, 74, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 8, 73, 16)         1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8, 73, 16)         64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 4, 37, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 2368)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                75808     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 82,279\n",
      "Trainable params: 82,151\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 3720 samples, validate on 1595 samples\n",
      "Epoch 1/100\n",
      " - 29s - loss: 1.9270 - accuracy: 0.2599 - val_loss: 1.9713 - val_accuracy: 0.2213\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.97131, saving model to ./model/sos-01-1.9713.hdf5\n",
      "Epoch 2/100\n",
      " - 29s - loss: 1.6399 - accuracy: 0.3691 - val_loss: 1.6607 - val_accuracy: 0.3486\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.97131 to 1.66071, saving model to ./model/sos-02-1.6607.hdf5\n",
      "Epoch 3/100\n",
      " - 29s - loss: 1.4492 - accuracy: 0.4610 - val_loss: 1.5183 - val_accuracy: 0.4207\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.66071 to 1.51829, saving model to ./model/sos-03-1.5183.hdf5\n",
      "Epoch 4/100\n",
      " - 30s - loss: 1.2548 - accuracy: 0.5390 - val_loss: 1.3303 - val_accuracy: 0.5160\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.51829 to 1.33034, saving model to ./model/sos-04-1.3303.hdf5\n",
      "Epoch 5/100\n",
      " - 29s - loss: 1.0231 - accuracy: 0.6331 - val_loss: 1.3269 - val_accuracy: 0.5423\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33034 to 1.32688, saving model to ./model/sos-05-1.3269.hdf5\n",
      "Epoch 6/100\n",
      " - 29s - loss: 0.8008 - accuracy: 0.7274 - val_loss: 1.0932 - val_accuracy: 0.6282\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.32688 to 1.09319, saving model to ./model/sos-06-1.0932.hdf5\n",
      "Epoch 7/100\n",
      " - 28s - loss: 0.6251 - accuracy: 0.7935 - val_loss: 1.0703 - val_accuracy: 0.6577\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.09319 to 1.07027, saving model to ./model/sos-07-1.0703.hdf5\n",
      "Epoch 8/100\n",
      " - 28s - loss: 0.4693 - accuracy: 0.8503 - val_loss: 0.8850 - val_accuracy: 0.7097\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.07027 to 0.88500, saving model to ./model/sos-08-0.8850.hdf5\n",
      "Epoch 9/100\n",
      " - 29s - loss: 0.3512 - accuracy: 0.8997 - val_loss: 0.9464 - val_accuracy: 0.7009\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.88500\n",
      "Epoch 10/100\n",
      " - 28s - loss: 0.2695 - accuracy: 0.9285 - val_loss: 0.8363 - val_accuracy: 0.7505\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.88500 to 0.83635, saving model to ./model/sos-10-0.8363.hdf5\n",
      "Epoch 11/100\n",
      " - 29s - loss: 0.1936 - accuracy: 0.9546 - val_loss: 0.8848 - val_accuracy: 0.7266\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.83635\n",
      "Epoch 12/100\n",
      " - 28s - loss: 0.1452 - accuracy: 0.9715 - val_loss: 0.9012 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.83635\n",
      "Epoch 13/100\n",
      " - 28s - loss: 0.1414 - accuracy: 0.9734 - val_loss: 0.9529 - val_accuracy: 0.7618\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.83635\n",
      "Epoch 14/100\n",
      " - 28s - loss: 0.0932 - accuracy: 0.9911 - val_loss: 0.7627 - val_accuracy: 0.8075\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.83635 to 0.76273, saving model to ./model/sos-14-0.7627.hdf5\n",
      "Epoch 15/100\n",
      " - 28s - loss: 0.0770 - accuracy: 0.9949 - val_loss: 0.8689 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.76273\n",
      "Epoch 16/100\n",
      " - 28s - loss: 0.1016 - accuracy: 0.9833 - val_loss: 0.9669 - val_accuracy: 0.7655\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.76273\n",
      "Epoch 17/100\n",
      " - 28s - loss: 0.1157 - accuracy: 0.9772 - val_loss: 1.2877 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.76273\n",
      "Epoch 18/100\n",
      " - 28s - loss: 0.1395 - accuracy: 0.9653 - val_loss: 0.9510 - val_accuracy: 0.7837\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.76273\n",
      "Epoch 19/100\n",
      " - 28s - loss: 0.0793 - accuracy: 0.9892 - val_loss: 0.9109 - val_accuracy: 0.7912\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.76273\n",
      "Epoch 20/100\n",
      " - 28s - loss: 0.0671 - accuracy: 0.9949 - val_loss: 0.8818 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.76273\n",
      "Epoch 21/100\n",
      " - 28s - loss: 0.0622 - accuracy: 0.9960 - val_loss: 0.9545 - val_accuracy: 0.7950\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.76273\n",
      "Epoch 22/100\n",
      " - 28s - loss: 0.0639 - accuracy: 0.9946 - val_loss: 1.0413 - val_accuracy: 0.7505\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.76273\n",
      "Epoch 23/100\n",
      " - 29s - loss: 0.0583 - accuracy: 0.9944 - val_loss: 0.9062 - val_accuracy: 0.8113\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.76273\n",
      "Epoch 24/100\n",
      " - 28s - loss: 0.0574 - accuracy: 0.9965 - val_loss: 0.9332 - val_accuracy: 0.8082\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.76273\n",
      "2283/2283 [==============================] - 5s 2ms/step\n",
      "\n",
      "Loss: 0.9249\n",
      "\n",
      "Accuracy: 0.8108\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일 (학습 과정 설정)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.3,\n",
    "                    epochs=100, batch_size=32, verbose=2,\n",
    "                    callbacks=[cb_checkpoint, cb_early_stopping])\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('\\nLoss: {:.4f}'.format(score[0]))\n",
    "print('\\nAccuracy: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "model.save('dnn_sos_model.h5')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
